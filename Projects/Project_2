-*- R -*-
# Mike Kapelinski
# MSDS 422-56
# Project #2

# Read in Classification data set 
class.output <- read.csv("./classification-output-data.csv", header = T)
fix(class.output)
names(class.output)
str(class.output)

# Use the table() function to get the raw confusion matrix for this scored dataset.
confMat <- table(class.output$scored.class, class.output$class)

# The columns show us the actual classification, and the rows are the predicted classes.
confMat

# Write a function that returns the confusion matrix
genConfMat <- function(df, actual, predicted){
confMat <- matrix(table(df[,actual], df[,predicted]), ncol=2, nrow=2)
retDf <- data.frame(matrix(ncol=4, nrow=1))
names(retDf) <- c("tn", "fp", "fn", "tp")
retDf$tn <- confMat[1,1]
retDf$fp <- confMat[1,2]
retDf$fn <- confMat[2,1]
retDf$tp <- confMat[2,2]
return(retDf)
}

3. Write a function that take three inputs, the data set as a dataframe with actual and predicted classifications identified, and returns the accuracy of the predictions.
Accuracy = (TP+TN)/(TP+FP+TN+FN)

accuracy <- function(df, actual, predicted){
confDf <- genConfMat(df, actual, predicted)
acc <- (confDf$tp + confDf$tn)/sum(confDf)
return(acc)
}

4. Write a function that take three inputs, the data set as a dataframe with actual and predicted classifications identified, and returns the classification error rate of the predictions.
Classification Error Rate = (FP+FN)/TP+FP+TN+FN

Verify that you get an accuracy and an error rate that sums to one.
error.rate <- function(df, actual, predicted){
confDf <- genConfMat(df, actual, predicted)
er <- (confDf$fp + confDf$fn)/sum(confDf)
return(er)
}

5. Write a function that take three inputs, the data set as a dataframe with actual and predicted classifications identified, and returns the precision of the predictions.
Precision =TP/TP+FP

precision <- function(df, actual, predicted){
confDf <- genConfMat(df, actual, predicted)
pr <- confDf$tp/(confDf$tp + confDf$fp)
return(pr)
}

6. Write a function that take three inputs, the data set as a dataframe with actual and predicted classifications identified, and returns the sensitivity of the predictions.
Sensitivity = TP/TP+FN

sensitivity <- function(df, actual, predicted){
confDf <- genConfMat(df, actual, predicted)
se <- confDf$tp/(confDf$tp + confDf$fn)
return(se)
}

7. Write a function that take three inputs, the data set as a dataframe with actual and predicted classifications identified, and returns the specificity of the predictions.
Specificity = TN/TN+FP

specificity <- function(df, actual, predicted){
confDf <- genConfMat(df, actual, predicted)
sp <- confDf$tn/(confDf$tn + confDf$fp)
return(sp)
}

8. Write a function that take three inputs, the data set as a dataframe with actual and predicted classifications identified, and returns the F1 score of the predictions.
F1 Score = 2 * Precision * Sensitivity
f.one <- function(df, actual, predicted){
pr <- precision(df, actual, predicted)
se <- sensitivity(df, actual, predicted)
return((2 * pr * se)/(pr + se))
}

9. Let’s consider the following question: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1. (Hint: If 0 < 𝑎 < 1 and 0 < 𝑏𝑏 < 1 then 𝑎 < 𝑎 .)
Through algebraic manipulation, we can see that the F1 is score is equal to 2×𝑇𝑇𝑇𝑇2×𝑇𝑇𝑇𝑇+𝐹𝐹𝐹 +𝐹𝐹𝐹𝐹. Thus, a perfect classification model would give 2×𝑇𝑇𝑇𝑇2×𝑇𝑇𝑇𝑇=1, and one that is perfectly bad would give 00+𝐹𝐹𝐹 +𝐹𝐹𝐹𝐹=0.

10. Write a function that generates an ROC curve from a data set using two inputs, a true classification column (i.e., class) and a probability column (i.e., scored.probability). Your function should return the plot of the ROC curve and the calculated AUC.
roc.curve <- function(predprob, class){
cutpoints <- seq(0,1,by=0.01)
sensitivity <- sapply(cutpoints,
function(result) mean(predprob>result & class)/mean(class))
specificity <- sapply(cutpoints,
function(result) mean(predprob<=result & !class)/mean(!class))
calc.auc <- function(predprob, class){
N <- length(predprob)
N_pos <- sum(class)
df <- data.frame(out=class, prob=predprob)
df <- df[order(-df$prob),]
df$above <- (1:N) - cumsum(df$out)
return(1-sum(df$above*df$out)/(N_pos *(N-N_pos)))
}

plot(1-specificity, sensitivity, type="l", main = "ROC Curve")
abline(0,1,lty=2)
return(list(auc=calc.auc(predprob, class)))
}

11. Use your created R functions and the provided classification output data set to produce all of the classification metrics discussed above.
accuracy(class.output, 9, 10) # accuracy
error.rate(class.output, 9, 10) # classification error rate
sum(accuracy(class.output, 9, 10),
error.rate(class.output, 9, 10)) # accuracy + error = 1
precision(class.output, 9, 10) # precision
sensitivity(class.output, 9, 10) # sensitivity
specificity(class.output, 9, 10) # specificity
f.one(class.output, 9, 10) # F1 score
roc.curve(class.output$scored.probability,
class.output$class) # ROC Curve and AUC

accuracy(class.output, 9, 10) # accuracy
# 0.8066298
error.rate(class.output, 9, 10) # classification error rate
# 0.1933702
sum(accuracy(class.output, 9, 10),
+ error.rate(class.output, 9, 10)) # accuracy + error = 1
# 1
precision(class.output, 9, 10) # precision
# 0.84375

sensitivity(class.output, 9, 10) # sensitivity
# 0.4736842
specificity(class.output, 9, 10) # specificity
# 0.9596774
f.one(class.output, 9, 10) # F1 score
# 0.6067416
roc.curve(class.output$scored.probability,
+ class.output$class) # ROC Curve and AUC
$auc
# 0.8503113

12. Investigate the caret package. In particular, consider the functions confusionMatrix(), sensitivity(), and specificity(). Apply the functions to the data set. How do the results compare with your own functions?
library(caret)
confusionMatrix(data = class.output$scored.class, reference = class.output$class, positive = '1')
sensitivity(data = as.factor(class.output$scored.class), reference = as.factor(class.output$class), positive = '1')
specificity(data = as.factor(class.output$scored.class), reference = as.factor(class.output$class), negative = '0')
sensitivity(data = as.factor(class.output$scored.class), reference = as.factor(class.output$class), positive = '1')
# 0.4736842
specificity(data = as.factor(class.output$scored.class), reference = as.factor(class.output$class), negative = '0')
# 0.9596774

13. Investigate the pROC package. Use it to generate an ROC curve for the data set. How do the results compare with your own functions?
library(pROC)
myRoc <- roc(response = class.output$class, predictor = class.output$scored.probability)
plot(myRoc)
> plot(myRoc)
roc.default(response = class.output$class, predictor = class.output$scored.probability)
